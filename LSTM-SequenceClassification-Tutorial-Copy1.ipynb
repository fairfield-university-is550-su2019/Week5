{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification with LSTM Recurrent Neural Networks with Keras\n",
    "\n",
    "Original post by [J. Brownlee](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/), editted by Dr. J. Tao."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence classification is a predictive modeling problem where you have some sequence of inputs over space or time and the task is to predict a category for the sequence. In any event, a *sentence* can be considered as a **sequence of words**.\n",
    "\n",
    "What makes this problem difficult includes:\n",
    "\n",
    "- the sequences can vary in length;\n",
    "    - traditional ML methods, including CNN, can only deal with fixed-length inputs; \n",
    "- be comprised of a very large vocabulary of input symbols; \n",
    "    - thus we have to deal with the *curse of dimensionality*; \n",
    "- and may require the model to learn the long-term context or dependencies between symbols in the input sequence; \n",
    "    - so far no other ML models can deal with such dependencies.\n",
    "\n",
    "In this tutorial, you will discover how you can develop LSTM recurrent neural network models for sequence classification problems in Python using the Keras deep learning library.\n",
    "\n",
    "If you are interested in sentence classification with CNN, [here](https://towardsdatascience.com/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-d2ee64b9dd0b) is a good post about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completion of this tutorial, you should be able to:\n",
    "\n",
    "- develop an LSTM model for a sequence classification problem.\n",
    "- reduce overfitting in your LSTM models through the use of dropout.\n",
    "- combine LSTM models with Convolutional Neural Networks that excel at learning spatial relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Step 1: Framing Your Analytical Problem\n",
    "\n",
    "The problem that we will use to demonstrate sequence learning in this tutorial is the [IMDB movie review sentiment classification problem](http://ai.stanford.edu/~amaas/data/sentiment/). Each movie review is a variable sequence of words and the sentiment of each movie review must be classified - hence, the reviews are the *unit of analysis* in this study.\n",
    "\n",
    "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a **positive** or **negative** sentiment. We are only playing with two sentiments here - of course you can have multi-class sentiment analysis (positive, negative, neutral, uncertain).\n",
    "\n",
    "The data was collected by [Stanford researchers and was used in a 2011 paper](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf) where a split of **50-50** of the data was used for training and testing. An accuracy of **88.89%** was achieved.\n",
    "\n",
    "Keras provides access to the *IMDB dataset built-in* - which means you do not have to download and import the dataset. The imdb.load_data() function allows you to load the dataset in a format that is ready for use in neural network and deep learning models.\n",
    "\n",
    "The words have been replaced by integers that indicate the ordered frequency of each word in the dataset. The sentences in each review are therefore comprised of a sequence of integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Step 1: Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np #### Python's numeric function package\n",
    "from keras.datasets import imdb #### Analysis dataset\n",
    "from keras.models import Sequential #### required layer in our LSTM network\n",
    "from keras.layers import Dense #### required layer in our LSTM network\n",
    "from keras.layers import LSTM #### required layer in our LSTM network\n",
    "from keras.layers.embeddings import Embedding #### required layer in our LSTM network\n",
    "from keras.preprocessing import sequence #### Packaged preprocessing step in Keras\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### I am using Keras 2.0.8, TensorFlow 1.3.0, please check your version here\n",
    "#### If your version is too low, you should use pip or Anaconda interface to update your Keras and TF packages\n",
    "#### Usually newer version would be fine\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "print('Your Keras version is:', keras.__version__)\n",
    "print('Your TensorFlow version is:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "You should have noticed that we imported a Keras layer called embedding - which would do the word embedding for us.\n",
    "\n",
    "**What is Word Embedding?**\n",
    "\n",
    "We will map each movie review into a real vector domain, a popular technique when working with text called word embedding. This is a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\n",
    "\n",
    "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer.\n",
    "\n",
    "We will map each word onto a 32 length real valued vector. We will also limit the total number of words that we are interested in modeling to the 5000 most frequent words, and zero out the rest. Finally, the sequence length (number of words) in each review varies, so we will constrain each review to be 500 words, truncating long reviews and pad the shorter reviews with zero values.\n",
    "\n",
    "Now that we have defined our problem and how the data will be prepared and modeled, we are ready to develop an LSTM model to classify the sentiment of movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Step 2: Loading data and Preprocessing\n",
    "\n",
    "We need to load the IMDB dataset. We are constraining the dataset to the top 5,000 words - hence, we are working our way down to deal with the 'curse of dimensionality'. By default, we also split the dataset into train (50%) and testing (50%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "We know that 50%/50% split is too harsh on training, maybe we want to use a more lenient 70/30 split for training/testing. \n",
    "\n",
    "Fill the following code block for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# fill-in your seed here, and un-comment the statement\n",
    "# seed = \n",
    "\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "# complete the following statement to split into 67% for train and 33% for test\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to truncate and pad the input sequences so that they are all the **same length** for modeling. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, **but same length vectors is required to perform the computation in Keras**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the shape of our data\n",
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"Testing data: \")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### You can look at what is in your data\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "#print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened to sentences? What are these numbers for? Type your answer below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "You may want to print out your newly-splited (train1, test1) datasets' shapes to check whether the split is correct.\n",
    "\n",
    "Fill-in the code block below for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the unique class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summarize number of classes: 0 - negative, 1 - positive\n",
    "print(\"Classes: \")\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very clean dataset, so split is the only preprocessing step we need to do. We can now move to the modeling (training) phase.\n",
    "\n",
    "*In your actual projects, preprocessing will take a lot of time - please refer to your experience in IS 540 for this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Step 3: Modeling/Training and Evaluation/Optimization\n",
    "\n",
    "We can now define, compile and fit our LSTM model.\n",
    "\n",
    "The first layer is the Embedded layer that uses 32 length vectors to represent each word. The next layer is the LSTM layer with 100 memory units (smart neurons). Finally, because this is a classification problem we use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in the problem.\n",
    "\n",
    "#### Evaluation Metric and Optimization Method\n",
    "Because it is a binary classification problem, log loss is used as the loss function ([**binary_crossentropy in Keras**](https://keras.io/losses/)). The efficient [ADAM optimization algorithm](https://keras.io/optimizers/) is also used. The model is fit for only 2 epochs because it quickly overfits the problem. A large batch size of 64 reviews is used to space out weight updates.\n",
    "\n",
    "Please notice that we also look at the classification accuracy at each epoch.\n",
    "\n",
    "Following step took about 25 minutes on my machine - so it is a good time to go for a bio-break, or grab a cup of coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %timeit\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "We claimed that the previous model will overfit, how did we know? (Type your answer below)\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Step 4: Testing/Deployment\n",
    "\n",
    "Once the model is fit, we estimate the performance of the model on unseen reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Remember we created our own training and testing datasets? Now let's put them to use. Fill in following code block for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_vecor_length = 32\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model1.add(LSTM(50)) #### less neurons will make training faster\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model1.summary())\n",
    "#Fill in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addendum: Fighting Overfitting\n",
    "### LSTM For Sequence Classification With Dropout\n",
    "\n",
    "Recurrent Neural networks like LSTM generally have the problem of overfitting.\n",
    "\n",
    "Dropout can be applied between layers using the Dropout Keras layer. We can do this easily by adding new Dropout layers between the Embedding and LSTM layers and the LSTM and Dense output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "model2.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see dropout having the desired impact on training with a slightly slower trend in convergence and in this case a lower final accuracy. The model could probably use a few more epochs of training and may achieve a higher skill (try it an see)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Add more epochs to above model to see if the result improves - also observe if the model **overfits**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "model2.fit(X_train, y_train, epochs=, batch_size=64) ####change number of epochs to 5\n",
    "# Final evaluation of the model\n",
    "scores = model2.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternately, dropout can be applied to the input and recurrent connections of the memory units with the LSTM precisely and separately.\n",
    "\n",
    "Keras provides this capability with parameters on the LSTM layer, the dropout for configuring the input dropout and recurrent_dropout for configuring the recurrent dropout. For example, we can modify the first example to add dropout to the input and recurrent connections as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding_vecor_length = 32\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model3.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model3.summary())\n",
    "model3.fit(X_train, y_train, epochs=5, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model3.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the LSTM specific dropout has a more pronounced effect on the convergence of the network than the layer-wise dropout. As above, the number of epochs was kept constant and could be increased to see if the skill of the model can be further lifted.\n",
    "\n",
    "Dropout is a powerful technique for combating overfitting in your LSTM models and it is a good idea to try both methods, but you may bet better results with the gate-specific dropout provided in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addendum 2: Using LSTM and CNN Together\n",
    "### LSTM and Convolutional Neural Network For Sequence Classification\n",
    "\n",
    "Convolutional neural networks excel at learning the spatial structure in input data.\n",
    "\n",
    "The IMDB review data does have a one-dimensional spatial structure in the sequence of words in reviews and the CNN may be able to pick out invariant features for good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer.\n",
    "\n",
    "We can easily add a one-dimensional CNN and max pooling layers after the Embedding layer which then feed the consolidated features to the LSTM. We can use a smallish set of 32 features with a small filter length of 3. The pooling layer can use the standard length of 2 to halve the feature map size.\n",
    "\n",
    "For example, we would create the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model5.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model5.add(MaxPooling1D(pool_size=2))\n",
    "####DROPOUT\n",
    "model5.add(LSTM(100)) ####EMBED DROPOUT\n",
    "model5.add(Dense(1, activation='sigmoid'))\n",
    "model5.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model5.summary())\n",
    "model5.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model5.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we achieve similar results to the first example although with less weights and faster training time.\n",
    "\n",
    "I would expect that even better results could be achieved if this example was further extended to use dropout.\n",
    "\n",
    "### YOUR TURN HERE\n",
    "\n",
    "Please try add dropout (layers or embedded in LSTM) in above model - and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial you discovered how to develop LSTM network models for sequence classification predictive modeling problems.\n",
    "\n",
    "Specifically, you learned:\n",
    "\n",
    "- How to develop a simple single layer LSTM model for the IMDB movie review sentiment classification problem.\n",
    "- How to extend your LSTM model with layer-wise and LSTM-specific dropout to reduce overfitting.\n",
    "- How to combine the spatial structure learning properties of a Convolutional Neural Network with the sequence learning of an LSTM.\n",
    "\n",
    "Please save your tutorial file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
